{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8VV6QBcQfvJ"
   },
   "source": [
    "# WORD PREDICTION IN N-GRAM LANGUAGE MODELLING USING STUPID BACKOFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT\n",
    "\n",
    "In order to predict the next word, N-gram language modelling technique is used with a smoothing method called _Stupid Backoff_ to calculate the relative frequencies of the word in n-gram and generate a score to identify the most likely next word. The model was evaluated using the concept of information theory by calculating the _shannon entropy_ and _perplexity_ to understand how certain/uncertain the model was while predicting the word which for our case gave average entropy 3 and average perplexity of just 31 which means the model was just uncertain for 31 alternative predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3S1e1TcTZuC"
   },
   "source": [
    "### INTRODUCTION\n",
    "\n",
    "Stupid Back-off model takes the maximum-likelihood estimator of each potential completed n-gram as the ratio between the number of occurrences of that n-gram in the training set and the number occurrences of the (n-1)-word “prefix” for this n-gram.  If this ratio is zero (i.e. the n-gram is not in the training set), the model “backs off” to look at the ratio between the n-gram and the (n-2)-word prefix obtained by lopping the first word from the original prefix. This process continues recursively, multiplying by a factor between 0 and 1 (say 'k') each time. It calculates this k parameter (heuristically set at a constant 0.4 in practice) to generate scores to identify the next most likely word. These scores are than renormalized to add up to 1 for all potential words that complete an n-gram and is listed in descending order to get the most likely word. Hence, there is no need to actually calculate the true probability of the next word as it is done in Katz back-off model. It can be evaluated on examining the perplexity and the concept of shannon extropy to see how well the prediction is which is based on the concept of information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HddZ1ekruqQ"
   },
   "source": [
    "### Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkmZapqU5IME"
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBVt8-phcEgG"
   },
   "source": [
    "### Data loading and Text Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-It16N9_5LJl"
   },
   "outputs": [],
   "source": [
    "# Reading N lines of data from input file\n",
    "N = 10000\n",
    "with open(\"ptb.txt\") as file:\n",
    "    lines = [next(file) for x in range(N)]\n",
    "joined_lines = [\" \".join(lines)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUPuLbHl7ZIE"
   },
   "outputs": [],
   "source": [
    "def clean_article(lines):\n",
    "  \n",
    "  '''\n",
    "  Returns a cleaned corpus of text after text preprocessing\n",
    "  \n",
    "  Input: lines which are present in the text corpus\n",
    "  \n",
    "  Output: Cleaned text without letters, replacing it with space and single letter pronoun like \"I\" or article like \"a\".\n",
    "  \n",
    "  '''\n",
    "  art1 = re.sub(\"[^A-Za-z]\", ' ', lines)          # substitutes a space for characters which are not in A-Za-z\n",
    "  art2 = re.sub(\"\\s[B-HJ-Zb-hj-z]\\s\", ' ', art1)  # substitutes a space for all characters EXCEPT A and I\n",
    "  art3 = re.sub(\"^[B-HJ-Zb-hj-z]\\s\", ' ', art2)   # substitutes a space for characters which does not begin with A-Za-z EXCEPT A and I\n",
    "  art4 = re.sub(\"\\s[B-HJ-Zb-hj-z]$\", ' ', art3)   # substitute a space for characters which ends with A-Za-z EXCEPT A and I\n",
    "  return art4.lower()                             # converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPm9Aopa7dHq"
   },
   "outputs": [],
   "source": [
    "# Tokenizes the words on whitespace with n-gram of length 1 to 5 and puts their counts into a Pandas dataframe with the n-grams as column names.\n",
    "# Converts a collection of text to a matrix of token counts\n",
    "ngram_bow = CountVectorizer(stop_words = None, preprocessor = clean_article, tokenizer = WhitespaceTokenizer().tokenize, ngram_range=(1,5), \n",
    "                            max_features = None, max_df = 1.0, min_df = 1, binary = False)\n",
    "\n",
    "# Fitting and transforming the model to create a sparse matrix of token counts\n",
    "ngram_count_sparse = ngram_bow.fit_transform(joined_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "GycBKBLtauUj",
    "outputId": "20a09dc6-6fd3-46b3-99eb-226599856584"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  5257\n",
       "1     4\n",
       "2     1\n",
       "3     1\n",
       "4     1"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the matrix to pandas dataframe\n",
    "ngram_count = pd.DataFrame(ngram_count_sparse.toarray())\n",
    "ngram_count.T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "TBNsCIz6apRV",
    "outputId": "f207445a-053f-4d03-c656-b9332e4385d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a diversified</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a diversified construction</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a diversified construction concern</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a french</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a french state</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a french state controlled</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a share</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a a share offer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0\n",
       "a                                     5257\n",
       "a a                                      4\n",
       "a a diversified                          1\n",
       "a a diversified construction             1\n",
       "a a diversified construction concern     1\n",
       "a a french                               1\n",
       "a a french state                         1\n",
       "a a french state controlled              1\n",
       "a a share                                1\n",
       "a a share offer                          1"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating column with feature names in n-gram\n",
    "ngram_count.columns = ngram_bow.get_feature_names()\n",
    "ngram_count.T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "K-Ylr8XBb38K",
    "outputId": "8e92cc99-b5f8-46d4-9cf8-0793f8bcb6b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>661747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.533743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.465793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12293.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  661747.000000\n",
       "mean        1.533743\n",
       "std        26.465793\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max     12293.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decriptive statistics\n",
    "ngram_count.T.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UKdBTCI8cqvC"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "1. The minimum count of word in the n-gram corpus is 1 and maximum being 12293\n",
    "2. Total n-gram count of word is 6,61,747 in the text corpus\n",
    "3. Average n-gram count is around 1.5 and each n-gram word is aproximately 26 standard deviations away from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "HfVW6rf27hLK",
    "outputId": "cc58d611-083a-43ea-bb42-76303f0b0033"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     12293\n",
       "unk     10913\n",
       "of       5871\n",
       "to       5610\n",
       "a        5257\n",
       "in       4401\n",
       "and      4118\n",
       "for      2157\n",
       "that     2157\n",
       "is       1760\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 10 most used words in the n-gram having count greater than 0\n",
    "ngram_sums = ngram_count.sum(axis = 0)\n",
    "ngram_sums = ngram_sums[ngram_sums > 0]\n",
    "ngram_sums.sort_values(axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPE0uMAMdups"
   },
   "outputs": [],
   "source": [
    "# Convert n-gram count dataframe into a Pandas series with the n-grams as indices for ease of working with the counts.  \n",
    "ngrams = list(ngram_sums.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfZEzrMbhT4p"
   },
   "source": [
    "### Stupid Back-off Language Modelling in N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sh0oORiq7oQF"
   },
   "outputs": [],
   "source": [
    "def count_of_onegrams(ngram_sums):\n",
    "  '''\n",
    "  Returns count of n-gram occurances as Integer\n",
    "  \n",
    "  Input: Words present in n-gram\n",
    "  \n",
    "  Output: Total number of occurances of 1-grams to calculate 1-gram frequency\n",
    "  '''\n",
    "  onegrams = 0\n",
    "  for ng in ngrams:\n",
    "    ng_split = ng.split(\" \")\n",
    "    if len(ng_split) == 1:\n",
    "      onegrams += ngram_sums[ng]\n",
    "  return onegrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGQ55iJh7rgK"
   },
   "outputs": [],
   "source": [
    "# This is the last resort of the back-off algorithm if the n-gram completion does not occur in the corpus with any of the prefix words.\n",
    "def base_freq(og):\n",
    "    '''\n",
    "    Returns frequencies of the 1-gram as Series\n",
    "  \n",
    "    Input: Words present in 1-gram\n",
    "  \n",
    "    Output: Series of 1-gram frequencies\n",
    "  '''\n",
    "    freqs = pd.Series()\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == 1:\n",
    "            freqs[ng] = ngram_sums[ng] / og\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VUnwQ_C7uSm"
   },
   "outputs": [],
   "source": [
    "# Calculating base freq for all onegrams\n",
    "bf = base_freq(count_of_onegrams(ngram_sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I27gbHL47xaX"
   },
   "outputs": [],
   "source": [
    "def calculate_scores(prefix, chops, factor = 0.4):\n",
    "    '''\n",
    "    The function below finds any n-grams that are completions of a given prefix phrase with a specified number (could be zero) of words 'chopped' off the beginning.  \n",
    "    \n",
    "    For each, it calculates the count ratio of the completion to the (chopped) prefix, tabulating them in a series to be returned by the function.  \n",
    "\n",
    "    If the number of chops equals the number of words in the prefix (i.e. all prefix words are chopped), the 1-gram base frequencies are returned.\n",
    "  \n",
    "    '''\n",
    "    cs = pd.Series()\n",
    "    prefix_split = prefix.split(\" \")\n",
    "    l = len(prefix_split)\n",
    "    prefix_split_chopped = prefix_split[chops:l]\n",
    "    new_l = l - chops\n",
    "    if new_l == 0:\n",
    "        return factor**chops * bf\n",
    "    prefix_chopped = ' '.join(prefix_split_chopped)\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if (len(ng_split) == new_l + 1) and (ng_split[0:new_l] == prefix_split_chopped):\n",
    "            cs[ng_split[-1]] = factor**chops * ngram_sums[ng] / ngram_sums[prefix_chopped]\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4Dc9TNLX76KM",
    "outputId": "1d181163-134c-4fef-ec36-ca8c1e322af0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of completion scores:\n",
    "calculate_scores('I am going', 0, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad0EVj5n8Np6"
   },
   "outputs": [],
   "source": [
    "def score_given(input, fact):\n",
    "    '''\n",
    "    The function takes different numbers of 'chops' up to the length of the prefix to output a combined list of scores for potential completion of input\n",
    "  \n",
    "    '''\n",
    "    sg = pd.Series()\n",
    "    given_split = input.split(\" \")\n",
    "    given_length = len(given_split)\n",
    "    for i in range(given_length+1):\n",
    "        fcs = calculate_scores(input, i, fact)\n",
    "        for i in fcs.index:\n",
    "            if i not in sg.index:\n",
    "                sg[i] = fcs[i]\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cR3er-GA8zZn"
   },
   "outputs": [],
   "source": [
    "def score_output(input, fact):\n",
    "    '''\n",
    "    This function takes potential completion scores as input, sorts in descending order and re-normalizes them as a percentage (pseudo-probability)\n",
    "    \n",
    "    '''\n",
    "    sg = score_given(input, fact)\n",
    "    ss = sg.sum()\n",
    "    sg = 100 * sg / ss\n",
    "    sg.sort_values(axis=0, ascending=False, inplace=True)\n",
    "    return round(sg,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dMRRbnA884sD",
    "outputId": "45af8e62-7025-4256-96ad-6ed7ece60baa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to         51.3\n",
       "back        4.3\n",
       "into        3.2\n",
       "unk         2.1\n",
       "the         1.8\n",
       "against     1.1\n",
       "down        1.1\n",
       "sour        1.1\n",
       "over        1.1\n",
       "on          1.1\n",
       "much        1.1\n",
       "like        1.1\n",
       "for         1.1\n",
       "through     1.1\n",
       "along       1.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample next word prediction scores\n",
    "score_output('I am going', 0.4)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNfUWWMYFE6A"
   },
   "source": [
    "### MODEL EVALUATION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3SImGy0Rmcaq"
   },
   "source": [
    "Using the concept of information theory, we can not only find out how well the model does with particular test prefixes (comparing predictions to actual completions), but also how uncertain it is given particular test prefixes (i.e. unlabeled data).\n",
    "\n",
    "Let us think about the case where the model predicts all of the training 1-grams (let’s say there is M of them) with equal probability. hen, we could place all of the 1-grams in a binary tree, and then by asking log (base 2) of M questions of someone who knew the actual completion, we could find the correct prediction. This quantity (log base 2 of M) is known as **entropy (symbol H)** and in general is defined as H = - ∑ (p_i * log(p_i)) where i goes from 1 to M and p_i is the predicted probability score for 1-gram i. (If p_i is always 1/M, we have H = -∑((1/M) * log(1/M)) for i from 1 to M. This is just M * -((1/M) * log(1/M)), which simplifies to -log(1/M), which further simplifies to log(M).) Entropy is expressed in bits (if the log chosen is base 2) since it is the number of yes/no questions needed to identify a word. If some of the p_i values are higher than others, entropy goes down since we can structure the binary tree to place more common words in the top layers, thus finding them faster as we ask questions.\n",
    "\n",
    "To encapsulate uncertainty of the model, we can use a metric called **perplexity**, which is 2^H, as calculated for a given test prefix. We can then take the average perplexity over the test prefixes to evaluate our model. In special case of equal probabilities assigned to each prediction, perplexity would be 2^log(M), i.e. just M. This means that perplexity is at most M, i.e. the model is “M-ways uncertain.” It can’t make a choice among M alternatives. If the probabilities are less uniformly distributed, entropy (H) and thus perplexity is lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pxcJ_VqQAbNr",
    "outputId": "79775d43-c306-4e33-cc35-5a513470e73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56909, 5210)\n"
     ]
    }
   ],
   "source": [
    "# Read N lines from the test data set\n",
    "N = 1000\n",
    "with open(\"ptb.test.txt\", \"r+\") as text:\n",
    "  articles_test = [next(text) for x in range(N)]\n",
    "  article = text.read().split()\n",
    "joined_articles_test = [\" \".join(articles_test)]\n",
    "# Converts a collection of text to a matrix of token counts\n",
    "count_vect = CountVectorizer(input=\"file\")\n",
    "docs_new = [ StringIO(x) for x in article ]\n",
    "# Fitting and transforming the model to create a sparse matrix of token counts\n",
    "X_train_counts = count_vect.fit(docs_new)\n",
    "len(X_train_counts.vocabulary_)\n",
    "article_transformed = X_train_counts.transform(docs_new)\n",
    "# Get the shape of transformed article\n",
    "print (article_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBLu5QhqI3hE"
   },
   "outputs": [],
   "source": [
    "# Tokenize test words into n-grams of length 5\n",
    "\n",
    "# Tokenizes the words on whitespace with n-gram of length 5 and puts their counts into a Pandas dataframe with the n-grams as column names.\n",
    "# Converts a collection of text to a matrix of token counts\n",
    "test_bow = CountVectorizer(stop_words = None, preprocessor = clean_article, tokenizer = WhitespaceTokenizer().tokenize, ngram_range=(5,5), \n",
    "                           max_features = None, max_df = 1.0, min_df = 1, binary = False)\n",
    "\n",
    "# Fitting and transforming the model to create a sparse matrix of token counts\n",
    "ngram_count_sparse_test = test_bow.fit_transform(joined_articles_test)\n",
    "\n",
    "# Converting matrix to pandas dataframe\n",
    "test_ngram_count = pd.DataFrame(ngram_count_sparse_test.toarray())\n",
    "\n",
    "# Creating a column containing ngram words as feature vector\n",
    "test_ngram_count.columns = test_bow.get_feature_names()\n",
    "\n",
    "# Find most used words in ngram having count greater than 1\n",
    "test_sums = test_ngram_count.sum(axis = 0)\n",
    "test_sums = test_sums[test_sums > 1]\n",
    "\n",
    "# Convert n-gram count dataframe into a Pandas series with the n-grams as indices for ease of working with the counts.  \n",
    "test_ngrams = list(test_sums.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrGh_a-PFYbA"
   },
   "outputs": [],
   "source": [
    "# The helper functions below give the number of occurrences of n-grams in order to explore and calculate frequencies\n",
    "def number_of_unique_ngrams(n, ngrams):\n",
    "    '''\n",
    "    Returns count of unique n-gram occurances as Integer\n",
    "  \n",
    "    Input: Words present in n-gram\n",
    "  \n",
    "    Output: Total number of occurances of unique n-grams to calculate n-gram frequency\n",
    "    '''\n",
    "    grams = 0\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == n:\n",
    "            grams += 1\n",
    "    return grams\n",
    "\n",
    "def number_of_ngrams(n, ngrams):\n",
    "    '''\n",
    "    Returns count of n-gram occurances as Integer\n",
    "  \n",
    "    Input: Words present in n-gram\n",
    "  \n",
    "    Output: Total number of occurances of n-grams to calculate n-gram frequency\n",
    "    '''\n",
    "    grams = 0\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == n:\n",
    "            grams += ngram_sums[ng]\n",
    "    return grams\n",
    "\n",
    "# This is the last resort of the back-off algorithm if the n-gram completion does not occur in the corpus with any of the prefix words.\n",
    "def base_freq(n, ngrams):\n",
    "    '''\n",
    "    Returns frequencies of the 1-gram as Series\n",
    "  \n",
    "    Input: Words present in 1-gram\n",
    "  \n",
    "    Output: Series of 1-gram frequencies\n",
    "    '''\n",
    "    tot_ngrams = number_of_ngrams(n, ngrams)\n",
    "    freqs = pd.Series()\n",
    "    for ng in ngrams:\n",
    "        ng_split = ng.split(\" \")\n",
    "        if len(ng_split) == n:\n",
    "            freqs[ng] = ngram_sums[ng] / tot_ngrams\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j143yvk5F2lC"
   },
   "outputs": [],
   "source": [
    "# Calculating base freq for all ngrams so as not to re-calculate multiple times:\n",
    "bf = base_freq(1, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWal7sjzT-v5"
   },
   "outputs": [],
   "source": [
    "def score_output(input, fact):\n",
    "    '''\n",
    "    This function takes potential completion scores as input, sorts in descending order and re-normalizes them as a percentage (pseudo-probability)\n",
    "    \n",
    "    '''\n",
    "    sg = score_given(input, fact)\n",
    "    ss = sg.sum()\n",
    "    sg = sg / ss\n",
    "    sg.sort_values(axis=0, ascending=False, inplace=True)\n",
    "    return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QD6C0NdUKga"
   },
   "outputs": [],
   "source": [
    "# Random generation of test samle\n",
    "num_test_grams = len(test_ngrams)\n",
    "import random\n",
    "r = random.sample(range(num_test_grams), 75)\n",
    "test_gram_sample = [test_ngrams[i] for i in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8sZK3yXGK49"
   },
   "outputs": [],
   "source": [
    "# create empty lists\n",
    "entropies = []\n",
    "top1 = []\n",
    "top3 = []\n",
    "top10 = []\n",
    "ranks = []\n",
    "\n",
    "for gram in test_gram_sample:\n",
    "    words = gram.split(' ')\n",
    "    ending_word = words[4]\n",
    "    pref = ' '.join(words[0:4])\n",
    "    so = score_output(pref, 0.4)\n",
    "    # calculate the entropy\n",
    "    entropy = sum(-1 * so * np.log2(so))\n",
    "    entropies.append(entropy)\n",
    "    # completion score of top 1,3,10 predicted words\n",
    "    top1.append(so.index[0] == ending_word)\n",
    "    top3.append(ending_word in so.index[0:3])\n",
    "    top10.append(ending_word in so.index[0:10])\n",
    "    if ending_word in bf.index:\n",
    "        ranks.append(so.index.get_loc(ending_word) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1D8D5667YhHB",
    "outputId": "7a0c52ee-f4f2-4b9e-f238-5ec4ba376f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4666666666666667"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completion score of top-1 predicted word\n",
    "sum(top1)/len(top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gqPC9v24JlhO",
    "outputId": "581ed31e-fd1a-4b1b-9ff5-2f808ee7f3e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completion score of Top-3 predicted word\n",
    "sum(top3)/len(top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aWjjV-jmJqO5",
    "outputId": "687da044-13e1-4d1b-ff30-5ba2d8a78b4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Completion score of Top-10 predicted word\n",
    "sum(top10)/len(top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KvqNeh4RJvij",
    "outputId": "59c62ce2-17d1-4540-d9ef-d626835d348a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.305318748986459"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average entropy\n",
    "np.mean(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yBL02rTNJvzO",
    "outputId": "4ad2ac06-5f6c-4b6d-b166-97e79ea6c5aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.873541749007823"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy deviance\n",
    "np.std(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MwWNJA2YOpf"
   },
   "outputs": [],
   "source": [
    "# Calculate perplexity which is 2^entropy\n",
    "perplexities = []\n",
    "for ent in entropies:\n",
    "    perplexities.append(2**ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EP2CLqdWJ7cA",
    "outputId": "0056fec6-5c20-4942-a90c-10f0dd345b23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.409549095646337"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average perplexity\n",
    "np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Vn6tJa4QYRbB",
    "outputId": "39a33eae-3244-442a-f3d6-a806d34d1678"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.78346306421152"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perplexity deviance\n",
    "np.std(perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVOp24hZAiCk"
   },
   "source": [
    "### CONCLUSION/RESULT:\n",
    "\n",
    "Word prediction algorithm has been implemented using the Stupid Back-off smoothing technique in N-gram Language modelling. The training text was count vectorized into 1-, 2-, 3-, 4- and 5-grams (of which there were many instances, including repeats) and then pruned to keep only those n-grams that appeared more than twice. The test set was count-vectorized only into 5-grams that appeared more than once. Reason being, the final word of a 5-gram that appears more than once in the test set is a bit easier to predict than that of a 5-gram that appears only once. On evaluating the model, we get average entropy of around 3 which gives average perplexity of just 31 i.e on an average, the model was just uncertain among 31 alternative predictions, which is very good for natural-language models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTRIBUTION:\n",
    "\n",
    "Personal: 70%, External references: 30%\n",
    "\n",
    "1. Understanding the concept of N-gram language modelling and its use in word prediction\n",
    "2. Implementation based on the new smoothing technique called Stupid Backoff introduced by Google in 2007 \n",
    "3. Incorporated initial text pre-processing that has text cleaning and exploratory data analysis\n",
    "4. Code based on the Penn Tree Bank dataset used for our word prediction\n",
    "5. Understanding and implementing the concept of Information Theory; Shannon Entropy and Perplexity for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzKB2NRZAVSg"
   },
   "source": [
    "### CITATION/REFERRENCES:\n",
    "\n",
    "1. https://www.aclweb.org/anthology/D07-1090.pdf\n",
    "2. https://en.wikipedia.org/wiki/Katz%27s_back-off_model\n",
    "3. https://rpubs.com/pferriere/dscapreport\n",
    "4. https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf\n",
    "5. https://medium.com/@davidmasse8/using-perplexity-to-evaluate-a-word-prediction-model-8820cf3fd3aa\n",
    "6. https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf\n",
    "7. https://medium.com/@davidmasse8/predicting-the-next-word-back-off-language-modeling-8db607444ba9\n",
    "8. https://github.com/b-knight/Text-Prediction-App-with-RShiny-and-Swiftkey-COCA\n",
    "9. https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "10. https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mz8qkuevAZRP"
   },
   "source": [
    "### LICENSES:\n",
    "\n",
    "**The MIT license:**\n",
    "\n",
    "https://opensource.org/licenses/MIT  Copyright 2019 Ami Gandhi, Pratik Kadi, Krunal Nanda\n",
    "\n",
    "**The Creative Commons Attribution 3.0 License:**\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/80x15.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Stupid Back-off Model for Word Prediction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
